{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34377,"databundleVersionId":3220602,"sourceType":"competition"}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Space Titanic - Bedu Equipo 21\n#### *Proyecto de evaluación módulo 4*","metadata":{}},{"cell_type":"code","source":"import numpy as np  # NumPy: Biblioteca para cálculos numéricos y manejo de arrays\nimport pandas as pd  # Pandas: Manipulación y análisis de datos estructurados\nimport seaborn as sns  # Seaborn: Visualización de datos avanzada basada en Matplotlib\nimport matplotlib.pyplot as plt  # Matplotlib: Creación de gráficos y visualizaciones\n\nimport tensorflow as tf  # TensorFlow: Framework para aprendizaje automático y redes neuronales\nfrom tensorflow.keras.layers import Normalization, Dense, InputLayer  # Keras Layers: Componentes para construir redes neuronales\nfrom tensorflow.keras.losses import MeanAbsoluteError  # Keras Losses: Funciones de pérdida para entrenamiento de modelos\nfrom tensorflow.keras.optimizers import Adam  # Keras Optimizers: Algoritmos de optimización para entrenar modelos\nfrom tensorflow.keras.metrics import RootMeanSquaredError  # Keras Metrics: Métricas para evaluar modelos de aprendizaje automático\nfrom tensorflow.keras.regularizers import l1_l2  # Keras Regularizers: Regularización para prevenir el sobreajuste en modelos\nfrom tensorflow.keras.models import Sequential  # Keras Models: Modelo secuencial para construir redes neuronales de forma lineal\nfrom tensorflow.keras.callbacks import EarlyStopping  # Keras Callbacks: Detención anticipada para evitar sobreajuste durante el entrenamiento\nfrom keras.layers import Dropout  # Keras Layers: Dropout para reducir el sobreajuste en redes neuronales\n\nfrom sklearn.metrics import roc_curve, auc  # Scikit-learn Metrics: Curva ROC y AUC para evaluación de modelos de clasificación\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay  # Scikit-learn Metrics: Matriz de confusión para evaluación de modelos\nfrom sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay  # Scikit-learn Metrics: Curva de precisión-recuperación\n\nfrom scipy.stats import norm  # SciPy Stats: Funciones y distribuciones estadísticas, aquí distribución normal\n\nfrom sklearn.model_selection import KFold\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:47.222151Z","iopub.execute_input":"2023-12-13T07:13:47.222786Z","iopub.status.idle":"2023-12-13T07:13:51.448719Z","shell.execute_reply.started":"2023-12-13T07:13:47.222751Z","shell.execute_reply":"2023-12-13T07:13:51.447815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preparación de los datos de Entrenamiento","metadata":{}},{"cell_type":"code","source":"# Cargando el dataset de entrenamiento y prueba\n\ndf_train_raw = pd.read_csv(\"/kaggle/input/spaceship-titanic/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/spaceship-titanic/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:51.450409Z","iopub.execute_input":"2023-12-13T07:13:51.451027Z","iopub.status.idle":"2023-12-13T07:13:51.508163Z","shell.execute_reply.started":"2023-12-13T07:13:51.450998Z","shell.execute_reply":"2023-12-13T07:13:51.507050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mostrando la extensión de cada dataset\n\nprint(len(df_train_raw))\nprint(len(df_test))","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:51.509496Z","iopub.execute_input":"2023-12-13T07:13:51.509816Z","iopub.status.idle":"2023-12-13T07:13:51.515329Z","shell.execute_reply.started":"2023-12-13T07:13:51.509788Z","shell.execute_reply":"2023-12-13T07:13:51.514395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Primeras 10 filas del set de entrenamiento\n\ndf_train_raw.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:51.518100Z","iopub.execute_input":"2023-12-13T07:13:51.518412Z","iopub.status.idle":"2023-12-13T07:13:51.550543Z","shell.execute_reply.started":"2023-12-13T07:13:51.518386Z","shell.execute_reply":"2023-12-13T07:13:51.549588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tipos de datos\ndata_type = df_train_raw.dtypes\nunique_values = df_train_raw.nunique()\nprint(data_type, \"\\n\")\n\nprint(unique_values)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:51.551902Z","iopub.execute_input":"2023-12-13T07:13:51.552243Z","iopub.status.idle":"2023-12-13T07:13:51.570109Z","shell.execute_reply.started":"2023-12-13T07:13:51.552217Z","shell.execute_reply":"2023-12-13T07:13:51.569095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualización de la cantidad de valores nulos que se tienen en el dataset de entrenamiento\n\ndf_train_raw.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:51.571551Z","iopub.execute_input":"2023-12-13T07:13:51.571896Z","iopub.status.idle":"2023-12-13T07:13:51.585415Z","shell.execute_reply.started":"2023-12-13T07:13:51.571842Z","shell.execute_reply":"2023-12-13T07:13:51.584498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Columnas sin relevancia\n\nEn el análisis realizado, se identificaron varias columnas como irrelevantes para el estudio. Esta irrelevancia se debe principalmente a dos razones: la presencia de una gran cantidad de datos únicos, tanto numéricos como no numéricos, que están asociados a servicios específicos; y la existencia de un alto número de datos nulos. Las columnas que caen en esta categoría son:\n\n- `Cabin`\n- `VIP`\n- `RoomService`\n- `Foodcourt`\n- `ShoppingMall`\n- `Spa`\n- `VRdeck`\n- `Name`\n\nNo obstante, es importante destacar una excepción significativa: la columna `Cabin`. Dado el contexto del problema, donde la nave espacial atraviesa una anomalía espaciotemporal, el análisis de esta columna podría ofrecer insights valiosos. Las anomalías espaciotemporales son fenómenos poco comprendidos, lo que nos lleva a plantear hipótesis y cuestionamientos sobre su funcionamiento en relación con los pasajeros de la nave. Algunas preguntas relevantes podrían ser:\n\n- ¿Influye la ubicación de los pasajeros en la nave (indicada por su cabina) en la probabilidad de ser transportados a través de la anomalía?\n- ¿Hay una correlación entre la edad de los pasajeros y la tendencia a ser transportados?\n- ¿El monto de dinero gastado por los viajeros en servicios de la nave tiene alguna relevancia en su transporte a través de la anomalía?\n\nGracias a estas preguntas, se hace relevante considerar la ubicación de los pasajeros en la nave, específicamente según la cabina en la que se encuentran. Un dato importante a tener en cuenta es que las cabinas se dividen en dos grupos principales:\n\n- Aquellas ubicadas en la sección **\"P\"** (Port), que corresponde al lado izquierdo de la nave.\n- Aquellas en la sección **\"S\"** (Starboard), que se refiere al lado derecho de la nave.\n\nEsta clasificación de cabinas en \"Port\" y \"Starboard\" podría ser crucial para entender cómo la posición en la nave afecta la experiencia de los pasajeros durante la anomalía espaciotemporal. Podría existir una correlación entre la ubicación de la cabina y la probabilidad de ser afectado por la anomalía, lo cual abre nuevas vías de análisis en nuestro estudio.\n\nLa estructura de la columna `Cabin` tiene el formato `deck/num/side`, pero para simplificar el análisis y evitar el manejo de 6560 datos distintos, hemos decidido dividir esta información en dos categorías únicas: **Port (P)** y **Starboard (S)**. Esta división se complementa con la información sobre el sueño criogénico de los pasajeros, permitiéndonos inferir si se encontraban dentro de su cabina en el momento de la transportación. Esto nos ayuda a analizar la probabilidad de que fueran transportados desde una determinada cabina en un lado específico de la nave mientras estaban en criosueño.\n\nLas demás columnas, como `RoomService`, `Foodcourt`, `ShoppingMall`, `Spa`, `VRdeck`, y `Name`, serán eliminadas de nuestro análisis. Estas no muestran una correlación aparente con el fenómeno de transportación que estamos estudiando. \n\nMás adelante, realizaremos una matriz de correlación para validar nuestras hipótesis acerca de la relación entre la ubicación de la cabina, el estado de criosueño de los pasajeros, y otras variables relevantes en nuestro estudio.\n","metadata":{}},{"cell_type":"code","source":"# Eliminar columnas \n\ndf_train_raw = df_train_raw.drop(columns=[\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"Name\", \"VIP\"])\n\n# Mostramos el nuevo dataset sin las columnas\n\ndf_train_raw.columns\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:51.586525Z","iopub.execute_input":"2023-12-13T07:13:51.586840Z","iopub.status.idle":"2023-12-13T07:13:51.599358Z","shell.execute_reply.started":"2023-12-13T07:13:51.586816Z","shell.execute_reply":"2023-12-13T07:13:51.598243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Análisis por columna\n\nEn esta sección, realizaremos una revisión detallada de cada columna para determinar el tratamiento de los datos nulos. Dependiendo de la naturaleza y la importancia de cada columna, decidiremos si eliminamos las filas con datos faltantes o si los rellenamos con un valor estadísticamente significativo, como la media o la moda.\n\n#### Columna `Age`\n\nLa columna `Age` representa la edad de los pasajeros. El primer paso es analizar la distribución de los datos de edad para entender su rango, media, mediana y posibles outliers. Esto nos ayudará a decidir si es más adecuado rellenar los datos faltantes con la media o la mediana, o si se requiere de alguna otra técnica de imputación.\n\n1. **Análisis Exploratorio**: Iniciaremos con un análisis exploratorio de la columna `Age`, utilizando estadísticas descriptivas y visualizaciones como histogramas o diagramas de caja para entender su distribución.\n2. **Tratamiento de Datos Nulos**: Según las características observadas, determinaremos el método más adecuado para tratar los datos n\n","metadata":{}},{"cell_type":"code","source":"# Asumimos que df_train_raw['Age'] contiene la edad y algunos datos nulos. \n\nmean_age = df_train_raw['Age'].mean()\nmedian_age = df_train_raw['Age'].median()\nmode_age = df_train_raw['Age'].mode()[0]  # mode() devuelve una Serie, obtenemos el primer valor en caso de datos multimodales\nstd_age = df_train_raw['Age'].std()\n\n# Graficando el histograma y la curva de distribución normal\nplt.figure(figsize=(10, 6))\n\n# Graficar la distribución de edades usando un histograma\nsns.histplot(df_train_raw['Age'].dropna(), bins=30, kde=False, color='blue', stat='density')\n\n# Graficar la curva de distribución normal\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mean_age, std_age)\nplt.plot(x, p, 'k', linewidth=2)\n\n# Trazar líneas verticales para la media, mediana y moda de las edades\nplt.axvline(mean_age, color='red', linestyle='dashed', linewidth=2)\nplt.axvline(median_age, color='green', linestyle='dashed', linewidth=2)\nplt.axvline(mode_age, color='orange', linestyle='dashed', linewidth=2)\n\n# Agregar texto para las edades media, mediana y moda\nplt.text(mean_age + 1, plt.ylim()[1] / 2, f'Media: {mean_age:.2f}', color='red')\nplt.text(median_age + 1, plt.ylim()[1] / 2, f'Mediana: {median_age:.2f}', color='green')\nplt.text(mode_age - 5, plt.ylim()[1] / 2, f'Moda: {mode_age:.2f}', color='orange')\n\n# Título y etiquetas\nplt.title('Distribución de la Edad vs. Distribución Normal')\nplt.xlabel('Edad')\nplt.ylabel('Densidad')\n\n# Mostrar el gráfico\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:51.600781Z","iopub.execute_input":"2023-12-13T07:13:51.601181Z","iopub.status.idle":"2023-12-13T07:13:52.253240Z","shell.execute_reply.started":"2023-12-13T07:13:51.601147Z","shell.execute_reply":"2023-12-13T07:13:52.252272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Rellenando Valores Nulos en la Columna de Edad con la Mediana\n\nUsar la mediana de la columna 'Edad' para llenar valores nulos está bien justificado por varias razones:\n\n1. **Robustez a Valores Atípicos**: La mediana es menos sensible a valores extremos en los datos. En un conjunto de datos donde la edad puede variar significativamente, algunas edades muy altas o muy bajas pueden sesgar la media, pero no tendrán el mismo efecto en la mediana. Esto hace que la mediana sea una medida más confiable para la tendencia central en este contexto.\n\n2. **Manejo de Distribución Sesgada**: Si la distribución de edades en tu conjunto de datos no es simétrica (es decir, está sesgada), la mediana proporciona una mejor representación de la tendencia central que la media. La media puede ser engañosa en distribuciones sesgadas, ya que se ve arrastrada hacia la cola.\n\n3. **Practicidad y Representatividad**: La mediana, al ser un valor real del conjunto de datos, a menudo tiene más sentido práctico. Representa un valor que se observa de manera realista en los datos, lo que puede no ser el caso con la media, especialmente si la media no es un número entero en el contexto de la edad.\n\n4. **Preservación de la Estructura de Datos**: Usar la mediana ayuda a mantener la distribución general de los datos de edad. Esto es particularmente importante en el modelado estadístico, ya que asegura que la integridad y la representatividad del conjunto de datos se mantengan. Al mantener la forma de la distribución constante, la mediana evita introducir cambios artificiales que podrían afectar los resultados de cualquier análisis o modelos construidos sobre el conjunto de datos.\n\nEn resumen, llenar los valores faltantes de edad con la mediana es una estrategia sólida, especialmente en casos donde se trata con valores atípicos, distribuciones sesgadas o se necesita mantener la integridad del conjunto de datos para un modelado preciso.\n","metadata":{}},{"cell_type":"code","source":"df_train_raw['Age'].fillna(df_train_raw['Age'].median(), inplace=True) # Rellenar los valores faltantes en la columna 'Age' con su mediana\".","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:52.254426Z","iopub.execute_input":"2023-12-13T07:13:52.254745Z","iopub.status.idle":"2023-12-13T07:13:52.261653Z","shell.execute_reply.started":"2023-12-13T07:13:52.254722Z","shell.execute_reply":"2023-12-13T07:13:52.260301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verificando que la columna 'Age' no tenga valores nulos.\ndf_train_raw.isna().sum() ","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:52.265523Z","iopub.execute_input":"2023-12-13T07:13:52.265877Z","iopub.status.idle":"2023-12-13T07:13:52.278903Z","shell.execute_reply.started":"2023-12-13T07:13:52.265833Z","shell.execute_reply":"2023-12-13T07:13:52.277887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Columnas HomePlanet, CryoSleep, Cabin, Destination\n\nPara las columnas `HomePlanet`, `CryoSleep`, `Cabin` y `Destination`, optaremos por eliminar las filas que contengan datos nulos. A diferencia de los datos numéricos como la edad, estas columnas contienen datos categóricos u objetos, donde la imputación de valores medios o medianas no es aplicable o no tiene sentido. Por lo tanto, la eliminación de filas con valores faltantes es una opción más adecuada para mantener la integridad y la precisión del análisis.\n\n1. **Evaluación de Impacto**: Antes de proceder con la eliminación, es crucial evaluar cuántas filas se verán afectadas. Esto implica visualizar la cantidad de datos nulos en cada una de estas columnas.\n2. **Decisión Basada en Datos**: Una vez comprendida la magnitud de los datos nulos, podremos tomar una decisión informada sobre si la eliminación de estas filas es viable sin comprometer significativamente la calidad y la cantidad del dataset.\n3. **Proceso de Eliminación**: Si la cantidad de filas a eliminar es razonable y no afecta de manera crítica el análisis, procederemos con la eliminación de estas filas.\n4. **Verificación Post-Eliminación**: Tras la eliminación, es importante realizar una verificación final para asegurarnos de que el proceso se ha completado correctamente y de que el dataset resultante sigue siendo adecuado para el análisis subsiguiente.\n\nEste proceso asegura que manejamos los datos nulos de una manera que respete la naturaleza de las variables categóricas y mantenga la calidad del análisis.\n","metadata":{}},{"cell_type":"code","source":"# Mostramos la cantidad de filas con datos nulos.\ndf_train_raw.isnull().any(axis=1).sum()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:52.280076Z","iopub.execute_input":"2023-12-13T07:13:52.280401Z","iopub.status.idle":"2023-12-13T07:13:52.295003Z","shell.execute_reply.started":"2023-12-13T07:13:52.280374Z","shell.execute_reply":"2023-12-13T07:13:52.294056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tras analizar las columnas `HomePlanet`, `CryoSleep`, `Cabin` y `Destination`, hemos identificado un total de 773 filas con datos nulos. Considerando que el tamaño total del dataset es de 8693 filas, estos datos nulos representan aproximadamente el 8.9% del dataset.\n\nDado que la eliminación de estos datos nulos afectará solo una pequeña fracción del dataset, hemos determinado que es viable proceder con su eliminación. Este porcentaje relativamente bajo sugiere que aún tendremos suficientes datos para entrenar un modelo de Machine Learning de manera efectiva. La eliminación de estas filas nos permitirá mantener la integridad y la relevancia de nuestro análisis sin comprometer significativamente la calidad del dataset.","metadata":{}},{"cell_type":"code","source":"# Elimina todas las filas que tengan al menos un dato nulo.\ndf_train_NoNulls = df_train_raw.dropna()\n\n# Verificando que el nuevo dataset no contenga valores nulos\ndf_train_NoNulls.isna().sum() \n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:52.296345Z","iopub.execute_input":"2023-12-13T07:13:52.296668Z","iopub.status.idle":"2023-12-13T07:13:52.318322Z","shell.execute_reply.started":"2023-12-13T07:13:52.296642Z","shell.execute_reply":"2023-12-13T07:13:52.317339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mostrando la nueva extensión del dataset de entrenamiento\n\nprint(len(df_train_NoNulls))","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:52.319542Z","iopub.execute_input":"2023-12-13T07:13:52.319886Z","iopub.status.idle":"2023-12-13T07:13:52.325200Z","shell.execute_reply.started":"2023-12-13T07:13:52.319838Z","shell.execute_reply":"2023-12-13T07:13:52.324186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Una vez que hemos eliminado las filas con datos nulos, el siguiente paso es revisar nuevamente los tipos de datos en nuestro dataset. El objetivo de esta revisión es asegurarnos de que los tipos de datos sean los más adecuados para el análisis que vamos a realizar y, si es necesario, modificar la estructura de los datos para optimizarla.\n\nEspecíficamente, buscaremos transformar ciertas columnas para que contengan valores booleanos y numéricos. Esto se debe a que muchos algoritmos de Machine Learning funcionan mejor con datos numéricos y pueden interpretar más fácilmente los valores booleanos. Por ejemplo:\n\n- Convertir variables categóricas en valores booleanos (True/False) cuando sea posible. Esto es particularmente útil para columnas que contienen dos categorías claramente definidas.\n- Transformar variables categóricas con múltiples categorías en variables numéricas mediante técnicas como la codificación one-hot, para representarlas de manera más efectiva en modelos numéricos.\n- Asegurarnos de que todas las columnas numéricas tengan el tipo de dato correcto (por ejemplo, `int` o `float`), para facilitar su procesamiento en los modelos de Machine Learning.\n\nEsta etapa es crucial para preparar nuestros datos para el modelado, asegurando que estén en un formato que permita la máxima eficiencia y efectividad en el entrenamiento de modelos.\n","metadata":{}},{"cell_type":"code","source":"# Tipos de datos\ndata_type = df_train_NoNulls.dtypes\nunique_values = df_train_NoNulls.nunique()\nprint(data_type, \"\\n\")\n\nprint(unique_values)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:52.326486Z","iopub.execute_input":"2023-12-13T07:13:52.326812Z","iopub.status.idle":"2023-12-13T07:13:52.348756Z","shell.execute_reply.started":"2023-12-13T07:13:52.326766Z","shell.execute_reply":"2023-12-13T07:13:52.347733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Análisis de la Columna `Cabin`\n\nLa columna `Cabin` es una de las más complejas dentro de nuestro dataset debido a su estructura única. Esta columna sigue el formato `deck/num/side`, donde `side` puede ser \"P\" (Port) o \"S\" (Starboard). Para simplificar el análisis y preparar los datos para el modelado, transformaremos esta parte de la columna en valores numéricos.\n\nProcederemos de la siguiente manera:\n\n1. **Separación de las Cabinas**: Primero, separamos la información de la columna `Cabin` en sus componentes individuales: `deck`, `num` y `side`.\n2. **Transformación de `side`**: Luego, convertiremos la parte `side` de la columna en valores numéricos. Asignaremos el valor `0` a las cabinas que terminen en \"P\" (Port) y el valor `1` a las que terminen en \"S\" (Starboard). Esta conversión nos permitirá utilizar esta información de manera más efectiva en modelos de Machine Learning.\n3. **Revisión y Verificación**: Tras la transformación, revisaremos los datos para asegurarnos de que la conversión se haya realizado correctamente y que los valores numéricos reflejen con precisión la información original de la columna `Cabin`.\n\nEste proceso de transformación es esencial para hacer que la información contenida en la columna `Cabin` sea más accesible y utilizable en nuestros análisis y modelos predictivos futuros.\n\n","metadata":{}},{"cell_type":"code","source":"df_train_NoNulls.head() # Estructura Actual","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:52.350128Z","iopub.execute_input":"2023-12-13T07:13:52.350470Z","iopub.status.idle":"2023-12-13T07:13:52.366647Z","shell.execute_reply.started":"2023-12-13T07:13:52.350442Z","shell.execute_reply":"2023-12-13T07:13:52.365621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utilizar .loc para transformar la columna 'Cabin' y manejar valores que no son cadenas\ndf_train_NoNulls.loc[:, 'Cabin'] = df_train_NoNulls['Cabin'].apply(\n    lambda x: \n        0 if isinstance(x, str) and x.endswith('P') else (  # Si x es una cadena y termina en 'P', devuelve 0\n        1 if isinstance(x, str) and x.endswith('S') else  # Si x es una cadena y termina en 'S', devuelve 1\n        x  # Si x no es una cadena, deja el valor como está\n    )\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:52.368002Z","iopub.execute_input":"2023-12-13T07:13:52.368384Z","iopub.status.idle":"2023-12-13T07:13:52.385089Z","shell.execute_reply.started":"2023-12-13T07:13:52.368354Z","shell.execute_reply":"2023-12-13T07:13:52.384182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_NoNulls.head() # Estructura 0 y 1 para cabin\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:52.386235Z","iopub.execute_input":"2023-12-13T07:13:52.386555Z","iopub.status.idle":"2023-12-13T07:13:52.404499Z","shell.execute_reply.started":"2023-12-13T07:13:52.386530Z","shell.execute_reply":"2023-12-13T07:13:52.403474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transformación de las Columnas `HomePlanet` y `Destination`\n\nPara las columnas `HomePlanet` y `Destination`, que contienen una cantidad limitada de valores únicos, optaremos por realizar un mapeo numérico en lugar de convertirlas en valores booleanos con la técnica de `get_dummies`. Aunque `get_dummies` es útil para transformar datos categóricos en un formato adecuado para modelos de Machine Learning, en este caso, preferimos evitar la creación de columnas adicionales para mantener la simplicidad y manejo del dataset.\n\nEl proceso para cada columna será el siguiente:\n\n1. **Mapeo Numérico de `HomePlanet`**: Asignaremos valores numéricos a cada uno de los planetas de origen únicos. Por ejemplo, podríamos asignar `0` a `Earth`, `1` a `Mars`, y `2` a `Europa`.\n2. **Mapeo Numérico de `Destination`**: De manera similar, asignaremos valores numéricos a cada destino único. Cada destino recibirá un número que lo represente.\n3. **Revisión y Verificación**: Después de realizar estos mapeos, revisaremos los datos para asegurarnos de que las transformaciones reflejen con precisión la información original de cada columna y que los nuevos valores numéricos sean coherentes y útiles para el análisis.\n\nEsta estrategia nos permite mantener un equilibrio entre la transformación eficiente de variables categóricas y la conservación de un dataset manejable y no excesivamente fragmentado.\n","metadata":{}},{"cell_type":"code","source":"valores_unicos_HomePlanet = df_train_NoNulls['HomePlanet'].unique()\nvalores_unicos_Destination = df_train_NoNulls['Destination'].unique()\n\nprint(valores_unicos_HomePlanet)\nprint(valores_unicos_Destination)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:52.405740Z","iopub.execute_input":"2023-12-13T07:13:52.406152Z","iopub.status.idle":"2023-12-13T07:13:52.416804Z","shell.execute_reply.started":"2023-12-13T07:13:52.406116Z","shell.execute_reply":"2023-12-13T07:13:52.415932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crear el mapeo de los nombres de los planetas a números\nmapeo_planetas = {'Mars': 2, 'Earth': 1, 'Europa': 0}\n\n# Eliminar espacios en blanco alrededor de los valores en la columna 'HomePlanet'\ndf_train_NoNulls.loc[:, 'HomePlanet'] = df_train_NoNulls['HomePlanet'].str.strip()\n\n# Aplicar el mapeo a la columna 'HomePlanet' en el DataFrame original usando .loc\ndf_train_NoNulls.loc[:, 'HomePlanet'] = df_train_NoNulls['HomePlanet'].replace(mapeo_planetas)\n\n# Crear el mapeo de los nombres de los destinos a números\nmapeo_destinos = {'TRAPPIST-1e': 2, 'PSO J318.5-22': 1, '55 Cancri e': 0}\n\n# Eliminar espacios en blanco alrededor de los valores en la columna 'Destination'\ndf_train_NoNulls.loc[:, 'Destination'] = df_train_NoNulls['Destination'].str.strip()\n\n# Aplicar el mapeo a la columna 'HomePlanet' en el DataFrame original usando .loc\ndf_train_NoNulls.loc[:, 'Destination'] = df_train_NoNulls['Destination'].replace(mapeo_destinos)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:52.418222Z","iopub.execute_input":"2023-12-13T07:13:52.418776Z","iopub.status.idle":"2023-12-13T07:13:52.450083Z","shell.execute_reply.started":"2023-12-13T07:13:52.418740Z","shell.execute_reply":"2023-12-13T07:13:52.449115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_NoNulls.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:52.451404Z","iopub.execute_input":"2023-12-13T07:13:52.452182Z","iopub.status.idle":"2023-12-13T07:13:52.465083Z","shell.execute_reply.started":"2023-12-13T07:13:52.452139Z","shell.execute_reply":"2023-12-13T07:13:52.464204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conversión de la Columna `PassengerId` a Valores Enteros\n\nLa columna `PassengerId` presenta un formato único `gggg_pp`, donde `gggg` representa el grupo al que pertenece el pasajero y `pp` es su número dentro del grupo. Aunque este formato proporciona información valiosa sobre la agrupación de los pasajeros, para fines de análisis y modelado de Machine Learning, es preferible convertir estos identificadores a valores numéricos. Esta conversión permitirá que los algoritmos procesen de manera más eficiente la información relacionada con los grupos de pasajeros.\n\nEl proceso de conversión será el siguiente:\n\n1. **Separación de Grupo y Número**: Primero, separaremos el `PassengerId` en dos componentes: el número del grupo (`gggg`) y el número del pasajero dentro del grupo (`pp`). \n2. **Conversión a Valores Numéricos**: Luego, convertiremos ambos componentes en valores enteros. Esto puede implicar tratar el número del grupo como un valor entero más significativo y el número del pasajero como un valor menos significativo, o combinarlos de alguna manera que preserve la relación entre ellos.\n3. **Creación de una Nueva Columna**: Podemos crear una nueva columna, como `PassengerGroupID`, que represente esta información numérica, permitiendo así una interpretación más clara de la relación entre los pasajeros y sus grupos.\n4. **Revisión y Verificación**: Tras la conversión, es importante revisar los nuevos valores numéricos para asegurarnos de que reflejan adecuadamente la relación original `gggg_pp` y que son útiles para el análisis.\n\nEste enfoque nos permitirá analizar las relaciones entre los pasajeros y sus grupos de una manera más cuantitativa y sistemática, facilitando el descubrimiento de patrones y correlaciones en el dataset.\n\n","metadata":{}},{"cell_type":"code","source":"# Crear una copia explícita del DataFrame para evitar la advertencia 'SettingWithCopyWarning'\ndf_train_NoNulls_copy = df_train_NoNulls.copy()\n\n# Realizar las operaciones en la copia\nsplit_columns = df_train_NoNulls_copy['PassengerId'].str.split('_', expand=True)\ndf_train_NoNulls_copy.loc[:, 'PassengerGroupID'] = split_columns[0].astype(int)\ndf_train_NoNulls_copy.loc[:, 'PassengerNumber'] = split_columns[1].astype(int)\n\n# Verificar los cambios\nprint(df_train_NoNulls_copy[['PassengerId', 'PassengerGroupID', 'PassengerNumber']])\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:52.466454Z","iopub.execute_input":"2023-12-13T07:13:52.467471Z","iopub.status.idle":"2023-12-13T07:13:52.497810Z","shell.execute_reply.started":"2023-12-13T07:13:52.467437Z","shell.execute_reply":"2023-12-13T07:13:52.496904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train_NoNulls_copy.copy()\ndf_train = df_train.drop( columns = \"PassengerId\")\n\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:52.499108Z","iopub.execute_input":"2023-12-13T07:13:52.499412Z","iopub.status.idle":"2023-12-13T07:13:52.516153Z","shell.execute_reply.started":"2023-12-13T07:13:52.499379Z","shell.execute_reply":"2023-12-13T07:13:52.515224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reposicionar las ultimas 2 columnas al inicio del dataset\n\ndf_train = df_train[df_train.columns[-2:].tolist() + df_train.columns[:-2].tolist()]\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:52.517672Z","iopub.execute_input":"2023-12-13T07:13:52.517998Z","iopub.status.idle":"2023-12-13T07:13:52.533848Z","shell.execute_reply.started":"2023-12-13T07:13:52.517972Z","shell.execute_reply":"2023-12-13T07:13:52.532762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Utilizamos el grafico heatmap y pasamos los datos a los parametros del grafico, con la finalidad de visualizar las correlaciones entre las variables\nax = sns.heatmap(df_train.corr(), vmin=-1, vmax=1, annot=True, cmap=\"YlGnBu\", linewidths=1);","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:52.535554Z","iopub.execute_input":"2023-12-13T07:13:52.535977Z","iopub.status.idle":"2023-12-13T07:13:53.109918Z","shell.execute_reply.started":"2023-12-13T07:13:52.535942Z","shell.execute_reply":"2023-12-13T07:13:53.108901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verificando que no haya valores nulos antes de crear los tensores de entrenamiento\n\ndf_train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:53.111256Z","iopub.execute_input":"2023-12-13T07:13:53.111564Z","iopub.status.idle":"2023-12-13T07:13:53.122676Z","shell.execute_reply.started":"2023-12-13T07:13:53.111537Z","shell.execute_reply":"2023-12-13T07:13:53.121784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creación de Tensores de Entrenamiento\n\nCon nuestro dataset ya limpio y compuesto solamente por valores booleanos y numéricos, el siguiente paso es la creación del tensor de entrenamiento. Este proceso es crucial para preparar los datos para su uso en modelos de Machine Learning. Los pasos a seguir son:\n\n1. **Definición de Tipos de Datos como Flotantes**: Todos los tipos de datos del dataset deben ser convertidos a flotantes. Esta conversión es importante porque la mayoría de los modelos de Machine Learning, en especial las redes neuronales, requieren de alta precisión en los cálculos, algo que los datos en formato flotante pueden ofrecer.\n\n2. **Mezcla de Datos del Tensor**: Mezclar los datos es un paso esencial para evitar sesgos en el modelo. Al realizar una mezcla aleatoria de las filas del dataset, nos aseguramos de que no haya patrones de aprendizaje basados en el orden de los datos, lo que podría afectar la capacidad del modelo para generalizar a nuevos datos.\n\n3. **Visualización de la Estructura del Tensor (Shape)**: Es importante visualizar la estructura del tensor creado. Conocer el shape del tensor es vital para entender cómo se alimentará al modelo de Machine Learning y para asegurar que la estructura de los datos es la adecuada para el proceso de entrenamiento.\n\nSiguiendo estos pasos, garantizamos que nuestro tensor de entrenamiento esté bien preparado y sea apto para ser utilizado en el entrenamiento de modelos de Machine Learning.\n\n\n","metadata":{}},{"cell_type":"code","source":"# Definir el tipo de dato del tensor de entrenamiento\ndf_float = df_train.astype('float32')\n\n# Convertir el dataset de entrenamiento a un tensor de entrenamiento\ntensor_data_train = tf.constant(df_float.values)\n\n# Mezcla del tensor de entrenamiento\ntensor_data_train = tf.random.shuffle(tensor_data_train)\n\n# Muestra los primeros 10 datos del tensor\nprint(tensor_data_train[:10])","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:53.123960Z","iopub.execute_input":"2023-12-13T07:13:53.124268Z","iopub.status.idle":"2023-12-13T07:13:54.385329Z","shell.execute_reply.started":"2023-12-13T07:13:53.124231Z","shell.execute_reply":"2023-12-13T07:13:54.384096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pasos para Preparar el Dataset de Prueba\n\nNecesitamos que el dataset de prueba esté alineado con el de entrenamiento. Aquí les dejo lo que hay que hacer, paso a paso:\n\n1. **Igualar Estructuras**: Asegúrense de que el dataset de prueba tenga la misma estructura que el de entrenamiento. Mismas columnas, mismos tipos de datos.\n\n2. **Tipos de Datos a Flotantes**: Convertir todos los datos a flotantes, igual que hicimos con el dataset de entrenamiento.\n\n3. **Chequeo de Estructura**: Denle un último vistazo y confirmen que todo está igual que en el de entrenamiento - número de columnas, tipos de datos, etc.\n\n4. **Revisión Final**: Antes de usarlo, revisen todo bien. Cualquier error en la estructura puede afectar los resultados del modelo.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## Preparación de los datos de Prueba","metadata":{}},{"cell_type":"code","source":"# Tipos de datos\ndata_type_test = df_test.dtypes\nunique_values_test = df_test.nunique()\nprint(data_type_test, \"\\n\")\n\nprint(unique_values_test)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:54.386837Z","iopub.execute_input":"2023-12-13T07:13:54.387592Z","iopub.status.idle":"2023-12-13T07:13:54.402563Z","shell.execute_reply.started":"2023-12-13T07:13:54.387550Z","shell.execute_reply":"2023-12-13T07:13:54.401668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualización de la cantidad de valores nulos que se tienen en el dataset de prueba\ndf_test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:54.410471Z","iopub.execute_input":"2023-12-13T07:13:54.410777Z","iopub.status.idle":"2023-12-13T07:13:54.422202Z","shell.execute_reply.started":"2023-12-13T07:13:54.410751Z","shell.execute_reply":"2023-12-13T07:13:54.421173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Eliminar columnas \ndf_test = df_test.drop(columns=[\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"Name\", \"VIP\"])\n\n# Mostramos el nuevo dataset sin las columnas\ndf_test.columns","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:54.423585Z","iopub.execute_input":"2023-12-13T07:13:54.424001Z","iopub.status.idle":"2023-12-13T07:13:54.434834Z","shell.execute_reply.started":"2023-12-13T07:13:54.423965Z","shell.execute_reply":"2023-12-13T07:13:54.433834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Asumimos que df_train_raw['Age'] contiene la edad y algunos datos nulos. \n\nmean_age_test = df_test['Age'].mean()\nmedian_age_test = df_test['Age'].median()\nmode_age_test = df_test['Age'].mode()[0]  # mode() devuelve una Serie, obtenemos el primer valor en caso de datos multimodales\nstd_age_test = df_test['Age'].std()\n\n# Graficando el histograma y la curva de distribución normal\nplt.figure(figsize=(10, 6))\n\n# Graficar la distribución de edades usando un histograma\nsns.histplot(df_test['Age'].dropna(), bins=30, kde=False, color='blue', stat='density')\n\n# Graficar la curva de distribución normal\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mean_age_test, std_age_test)\nplt.plot(x, p, 'k', linewidth=2)\n\n# Trazar líneas verticales para la media, mediana y moda de las edades\nplt.axvline(mean_age_test, color='red', linestyle='dashed', linewidth=2)\nplt.axvline(median_age_test, color='green', linestyle='dashed', linewidth=2)\nplt.axvline(mode_age_test, color='orange', linestyle='dashed', linewidth=2)\n\n# Agregar texto para las edades media, mediana y moda\nplt.text(mean_age_test + 1, plt.ylim()[1] / 2, f'Media: {mean_age_test:.2f}', color='red')\nplt.text(median_age_test + 1, plt.ylim()[1] / 2, f'Mediana: {median_age_test:.2f}', color='green')\nplt.text(mode_age_test - 5, plt.ylim()[1] / 2, f'Moda: {mode_age_test:.2f}', color='orange')\n\n# Título y etiquetas\nplt.title('Distribución de la Edad vs. Distribución Normal')\nplt.xlabel('Edad')\nplt.ylabel('Densidad')\n\n# Mostrar el gráfico\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:54.436342Z","iopub.execute_input":"2023-12-13T07:13:54.436753Z","iopub.status.idle":"2023-12-13T07:13:54.834217Z","shell.execute_reply.started":"2023-12-13T07:13:54.436718Z","shell.execute_reply":"2023-12-13T07:13:54.833240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['Age'].fillna(df_test['Age'].median(), inplace=True) # Rellenar los valores faltantes en la columna 'Age' con su mediana\".","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:54.835576Z","iopub.execute_input":"2023-12-13T07:13:54.835991Z","iopub.status.idle":"2023-12-13T07:13:54.842825Z","shell.execute_reply.started":"2023-12-13T07:13:54.835953Z","shell.execute_reply":"2023-12-13T07:13:54.841827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verificando que la columna 'Age' no tenga valores nulos.\ndf_test.isna().sum() ","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:54.844199Z","iopub.execute_input":"2023-12-13T07:13:54.844549Z","iopub.status.idle":"2023-12-13T07:13:54.857869Z","shell.execute_reply.started":"2023-12-13T07:13:54.844504Z","shell.execute_reply":"2023-12-13T07:13:54.856800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mostramos la cantidad de filas con datos nulos.\ndf_test.isnull().any(axis=1).sum()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:54.859134Z","iopub.execute_input":"2023-12-13T07:13:54.859458Z","iopub.status.idle":"2023-12-13T07:13:54.872243Z","shell.execute_reply.started":"2023-12-13T07:13:54.859409Z","shell.execute_reply":"2023-12-13T07:13:54.871184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utilizar .loc para transformar la columna 'Cabin' y manejar valores que no son cadenas\ndf_test.loc[:, 'Cabin'] = df_test['Cabin'].apply(\n    lambda x: \n        0 if isinstance(x, str) and x.endswith('P') else (  # Si x es una cadena y termina en 'P', devuelve 0\n        1 if isinstance(x, str) and x.endswith('S') else  # Si x es una cadena y termina en 'S', devuelve 1\n        x  # Si x no es una cadena, deja el valor como está\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:54.873611Z","iopub.execute_input":"2023-12-13T07:13:54.874345Z","iopub.status.idle":"2023-12-13T07:13:54.887259Z","shell.execute_reply.started":"2023-12-13T07:13:54.874305Z","shell.execute_reply":"2023-12-13T07:13:54.886370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head() # Estructura 0 y 1 para cabin","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:54.888405Z","iopub.execute_input":"2023-12-13T07:13:54.888693Z","iopub.status.idle":"2023-12-13T07:13:54.910088Z","shell.execute_reply.started":"2023-12-13T07:13:54.888669Z","shell.execute_reply":"2023-12-13T07:13:54.909025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valores_unicos_HomePlanet_test = df_test['HomePlanet'].unique()\nvalores_unicos_Destination_test = df_test['Destination'].unique()\n\nprint(valores_unicos_HomePlanet_test)\nprint(valores_unicos_Destination_test)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:54.911635Z","iopub.execute_input":"2023-12-13T07:13:54.912054Z","iopub.status.idle":"2023-12-13T07:13:54.920645Z","shell.execute_reply.started":"2023-12-13T07:13:54.912017Z","shell.execute_reply":"2023-12-13T07:13:54.919595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crear el mapeo de los nombres de los planetas a números\nmapeo_planetas_test = {'Mars': 2, 'Earth': 1, 'Europa': 0}\n\n# Eliminar espacios en blanco alrededor de los valores en la columna 'HomePlanet'\ndf_test.loc[:, 'HomePlanet'] = df_test['HomePlanet'].str.strip()\n\n# Aplicar el mapeo a la columna 'HomePlanet' en el DataFrame original usando .loc\ndf_test.loc[:, 'HomePlanet'] = df_test['HomePlanet'].replace(mapeo_planetas_test)\n\n# Crear el mapeo de los nombres de los destinos a números\nmapeo_destinos_test = {'TRAPPIST-1e': 2, 'PSO J318.5-22': 1, '55 Cancri e': 0}\n\n# Eliminar espacios en blanco alrededor de los valores en la columna 'Destination'\ndf_test.loc[:, 'Destination'] = df_test['Destination'].str.strip()\n\n# Aplicar el mapeo a la columna 'HomePlanet' en el DataFrame original usando .loc\ndf_test.loc[:, 'Destination'] = df_test['Destination'].replace(mapeo_destinos_test)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:54.921823Z","iopub.execute_input":"2023-12-13T07:13:54.922192Z","iopub.status.idle":"2023-12-13T07:13:54.944553Z","shell.execute_reply.started":"2023-12-13T07:13:54.922162Z","shell.execute_reply":"2023-12-13T07:13:54.943814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:54.945543Z","iopub.execute_input":"2023-12-13T07:13:54.945805Z","iopub.status.idle":"2023-12-13T07:13:54.958163Z","shell.execute_reply.started":"2023-12-13T07:13:54.945782Z","shell.execute_reply":"2023-12-13T07:13:54.957045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crear una copia explícita del DataFrame para evitar la advertencia 'SettingWithCopyWarning'\ndf_test_copy = df_test.copy()\n\n# Realizar las operaciones en la copia\nsplit_columns_test = df_test_copy['PassengerId'].str.split('_', expand=True)\ndf_test_copy.loc[:, 'PassengerGroupID'] = split_columns_test[0].astype(int)\ndf_test_copy.loc[:, 'PassengerNumber'] = split_columns_test[1].astype(int)\n\n# Verificar los cambios\nprint(df_test_copy[['PassengerId', 'PassengerGroupID', 'PassengerNumber']])","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:54.959509Z","iopub.execute_input":"2023-12-13T07:13:54.959822Z","iopub.status.idle":"2023-12-13T07:13:54.983511Z","shell.execute_reply.started":"2023-12-13T07:13:54.959795Z","shell.execute_reply":"2023-12-13T07:13:54.982416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df_test_copy.copy()\ndf_test = df_test.drop( columns = \"PassengerId\")\n\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:54.984730Z","iopub.execute_input":"2023-12-13T07:13:54.985142Z","iopub.status.idle":"2023-12-13T07:13:55.007179Z","shell.execute_reply.started":"2023-12-13T07:13:54.985111Z","shell.execute_reply":"2023-12-13T07:13:55.006127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reposicionar las ultimas 2 columnas al inicio del dataset\n\ndf_test = df_test[df_test.columns[-2:].tolist() + df_test.columns[:-2].tolist()]\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:55.008922Z","iopub.execute_input":"2023-12-13T07:13:55.009591Z","iopub.status.idle":"2023-12-13T07:13:55.025311Z","shell.execute_reply.started":"2023-12-13T07:13:55.009555Z","shell.execute_reply":"2023-12-13T07:13:55.024278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Utilizamos el grafico heatmap y pasamos los datos a los parametros del grafico, con la finalidad de visualizar las correlaciones entre las variables\nax = sns.heatmap(df_test.corr(), vmin=-1, vmax=1, annot=True, cmap=\"YlGnBu\", linewidths=1);","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:55.026672Z","iopub.execute_input":"2023-12-13T07:13:55.027005Z","iopub.status.idle":"2023-12-13T07:13:55.545541Z","shell.execute_reply.started":"2023-12-13T07:13:55.026977Z","shell.execute_reply":"2023-12-13T07:13:55.544408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verificar que no haya valores nulos antes de crear los tensores de prueba\ndf_test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:55.547065Z","iopub.execute_input":"2023-12-13T07:13:55.547675Z","iopub.status.idle":"2023-12-13T07:13:55.557119Z","shell.execute_reply.started":"2023-12-13T07:13:55.547637Z","shell.execute_reply":"2023-12-13T07:13:55.556189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Creación de Tensores de Prueba**","metadata":{}},{"cell_type":"code","source":"# Definir el tipo de dato del tensor de entrenamiento\ndf_test_float = df_test.astype('float32')\n\n# Convertir el dataset de entrenamiento a un tensor de entrenamiento\ntensor_data_test = tf.constant(df_test_float.values)\n\n# Muestra los primeros 10 valores del tensor\nprint(tensor_data_test[:10])","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:55.558359Z","iopub.execute_input":"2023-12-13T07:13:55.558681Z","iopub.status.idle":"2023-12-13T07:13:55.571507Z","shell.execute_reply.started":"2023-12-13T07:13:55.558654Z","shell.execute_reply":"2023-12-13T07:13:55.570407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Definición de los Valores x y y\n\nEn la preparación de nuestro modelo de Machine Learning, es esencial definir correctamente los conjuntos de datos `x` y `y`. Estos conjuntos representan, respectivamente, las variables independientes (predictores) y la variable dependiente (lo que queremos predecir).\n\n### Valores x\n- **Descripción**: Los valores `x` corresponden a todos los datos del dataset excepto la columna de supervivencia.\n- **Objetivo**: Estos valores son los predictores que nuestro modelo utilizará para aprender y hacer predicciones. Incluyen todas las características y atributos relevantes que se consideran influyentes para predecir el resultado.\n\n### Valores y\n- **Descripción**: Los valores `y` corresponden a los datos de la columna de supervivencia.\n- **Objetivo**: Esta es la variable objetivo que nuestro modelo intentará predecir. Representa el resultado de supervivencia, que es el foco principal de nuestro análisis.\n\nAl separar el dataset en estos dos conjuntos, podemos entrenar nuestro modelo de Machine Learning de manera efectiva, utilizando los valores `x` para predecir los valores `y`.\n","metadata":{}},{"cell_type":"code","source":"# Para los valores de x seleccionamos todas las columnas del tensor de entrenamiento que corresponden a los predictores, a excepción de la ultima que es la variable dependiente\n\nX = tensor_data_train[:, :-1] # Todos los valores de todas las columnas a excepción de la ultima\ny = tensor_data_train[:, -1] # Todos los valores de la ultima columna\nprint(X, \"/n\")\nprint(y)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:55.572997Z","iopub.execute_input":"2023-12-13T07:13:55.573427Z","iopub.status.idle":"2023-12-13T07:13:55.586723Z","shell.execute_reply.started":"2023-12-13T07:13:55.573377Z","shell.execute_reply":"2023-12-13T07:13:55.585712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Expansión de las Dimensiones de `y`\n\nActualmente, nuestro tensor `y` tiene una forma de (7920,), lo que indica que es unidimensional, mientras que `x` tiene una forma de (7920,7), siendo bidimensional. Es importante que todos los tensores utilizados en el modelado de Machine Learning tengan dimensiones consistentes. Por lo tanto, necesitamos transformar `y` para que también sea un tensor bidimensional. \n\n### Importancia de la Consistencia de Dimensiones en Machine Learning\n\n- **Compatibilidad**: Algunos frameworks y bibliotecas de Machine Learning requieren que tanto las características (features) como las etiquetas (labels) tengan la misma cantidad de dimensiones para garantizar la coherencia interna durante el entrenamiento.\n  \n- **Uniformidad**: Tener todos los datos, incluyendo los objetivos o etiquetas, con el mismo número de dimensiones simplifica el manejo y preprocesamiento de los datos.\n\n- **Requisitos del Modelo**: Ciertos modelos y algoritmos están diseñados para trabajar con entradas bidimensionales, tanto para las características como para los objetivos, incluso si el objetivo es un valor único por instancia.\n\n### Reshape de `y` a 2D\n\nPara cumplir con estos requisitos, es una práctica estándar en Machine Learning realizar un reshape de los arrays objetivo 1D a 2D. Este proceso asegura que nuestro tensor `y` sea compatible con los requisitos del modelo y las expectativas de los frameworks utilizados.\n","metadata":{}},{"cell_type":"code","source":"# Expandir dimensiones de 1D a 2D\ny = tf.expand_dims(y, axis = -1)\nprint(y[:5])","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:55.587796Z","iopub.execute_input":"2023-12-13T07:13:55.588153Z","iopub.status.idle":"2023-12-13T07:13:55.595786Z","shell.execute_reply.started":"2023-12-13T07:13:55.588126Z","shell.execute_reply":"2023-12-13T07:13:55.594908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normalización de Datos\n\nLa normalización de los datos es un paso crucial en la preparación de nuestro dataset para el modelado de Machine Learning. Este proceso implica escalar las características de entrada a un rango estándar, lo que puede mejorar significativamente el rendimiento de los modelos de aprendizaje automático. \n\n### Importancia de la Normalización de Datos\n\n- **Contribución Equitativa de Características**: Al normalizar los datos, aseguramos que todas las características contribuyan de manera equitativa al resultado. Sin normalización, las características con rangos más amplios podrían influir desproporcionadamente en el modelo, llevando a resultados sesgados.\n\n- **Mejora del Gradiente durante el Entrenamiento**: La normalización también es fundamental para garantizar que los gradientes estén bien escalados durante el entrenamiento. Esto facilita que el algoritmo de optimización (como el descenso del gradiente) converja más rápidamente y de manera más efectiva.\n\n### Proceso de Normalización\n\nEl proceso de normalización adaptativa implica ajustar las características de entrada para que tengan una media aproximada de 0 y una desviación estándar de 1. Esto se logra restando el valor medio de cada característica y dividiendo por su desviación estándar. Este enfoque es comúnmente conocido como normalización Z-score o estandarización.\n\nAl normalizar los datos de esta manera, no solo mejoramos el rendimiento del modelo, sino que también hacemos que el proceso de entrenamiento sea más estable y consistente, independientemente de las escalas de las características originales.\n","metadata":{}},{"cell_type":"code","source":"# Crear una instancia de la capa de Normalización\nnormalizador = Normalization()\n\n# \"Adaptar\" la capa de Normalización con nuestros datos de entrenamiento.\n# Esto permite que la capa aprenda las estadísticas (media y desviación estándar) de los datos.\n# En términos simples: le estamos diciendo a la capa \"Observa estos datos y aprende cómo suelen ser\".\nnormalizador.adapt(X)\n\n# Normalizar los datos 'X' utilizando la capa de normalización y mostrar los primeros 5 elementos.\n# Basándose en lo que ha aprendido, la capa transformará estos datos a una escala común.\ndatos_normalizados = normalizador(X)\ndatos_normalizados = normalizador(X)\ndatos_normalizados[:5]\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:55.597221Z","iopub.execute_input":"2023-12-13T07:13:55.597538Z","iopub.status.idle":"2023-12-13T07:13:56.162135Z","shell.execute_reply.started":"2023-12-13T07:13:55.597512Z","shell.execute_reply":"2023-12-13T07:13:56.161117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Funciones de Activación y Regularizadores\n\n**Funciones de Activación**\n\nLas funciones de activación son cruciales en las redes neuronales, ya que ayudan a propagar el gradiente y permiten que la red aprenda patrones complejos.\n\n**ReLU (Unidad Lineal Rectificada)**\n- Ideal para muchas capas ocultas en redes profundas debido a su simplicidad y capacidad de propagar el gradiente sin cambios.\n- Fórmula: \\( $f(x) = \\max(0, x) $\\)\n- Referencia: [ReLU y sus Variantes en Redes Neuronales](https://arxiv.org/abs/2012.03355)\n\n**Sigmoide**\n- Útil en la capa de salida para clasificación binaria, ya que sus valores están entre 0 y 1.\n- Fórmula: \\($ f(x) = \\frac{1}{1 + e^{-x}} $\\)\n- Referencia: [Redes Neuronales y Regresión Logística](http://www.hlt.utdallas.edu/~vgogate/ml/2019s/lectures/neural-networks.pdf)\n\n**Tanh**\n- Similar al sigmoide pero con un rango entre -1 y 1, a menudo se usa en capas ocultas.\n- Fórmula: \\( $f(x) = \\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$ \\)\n- Referencia: [Función Tangente Hiperbólica y sus Propiedades](https://people.math.sc.edu/girardi/m142/handouts/10s-tanh.pdf)\n\n**Softmax**\n- Comúnmente usada en la capa de salida para clasificación multiclase, ya que proporciona una distribución de probabilidades entre múltiples clases.\n- Fórmula: \\( $f(x)_i = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$ \\)\n- Referencia: [Understanding the Softmax Function](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer)\n\n\nLa regularización ayuda a prevenir el sobreajuste en el modelo añadiendo una penalización a los pesos durante el entrenamiento.\n\n**Regularización L1**\n- Útil cuando se sospecha que muchas características son irrelevantes o cuando se desea un modelo disperso.\n- Fórmula de Penalización: \\( $P = \\alpha \\sum |w| $\\)\n- Referencia: [Regularización L1](https://developers.google.com/machine-learning/glossary/#L1_regularization)\n\n**Regularización L2**\n- Ayuda a manejar la colinealidad y es a menudo preferible cuando todas las características son relevantes.\n- Fórmula de Penalización: \\($ P = \\alpha \\sum w^2 $\\)\n- Referencia: [Regularización L2](https://developers.google.com/machine-learning/glossary/#L2_regularization)\n\nDeterminar el número de neuronas y capas ocultas requiere experimentación y validación cruzada. Una red con una o dos capas ocultas puede aprender una amplia variedad de patrones; capas adicionales pueden ser útiles para datos más complejos.\n\nReferencia: [Recomendaciones prácticas para el entrenamiento basado en gradiente de arquitecturas profundas](https://arxiv.org/abs/1206.5533)\n\n\n","metadata":{}},{"cell_type":"code","source":"def crear_modelo():\n    # Inicializar el modelo\n    model = Sequential([\n        InputLayer(input_shape=(7,)),  # Ajusta el shape de entrada según tu set de entrenamiento\n        normalizador,  # Utiliza la capa de normalización que ya has definido\n        Dense(64, activation='relu'),  # Primera capa oculta\n        Dropout(0.2),  # Capa de dropout\n        Dense(32, activation='relu'),  # Segunda capa oculta\n        Dense(16, activation='relu'),  # Tercera capa oculta\n        Dense(1, activation='sigmoid')  # Capa de salida\n    ])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:56.163337Z","iopub.execute_input":"2023-12-13T07:13:56.163629Z","iopub.status.idle":"2023-12-13T07:13:56.169523Z","shell.execute_reply.started":"2023-12-13T07:13:56.163604Z","shell.execute_reply":"2023-12-13T07:13:56.168530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creación de Conjuntos de Datos para Entrenamiento, Validación y Prueba\n\nEn el desarrollo de modelos de Machine Learning, es una práctica estándar dividir el dataset principal en tres conjuntos distintos: de entrenamiento, de validación y de prueba. Cada uno de estos conjuntos tiene un propósito específico y es crucial para el desarrollo de un modelo robusto y bien ajustado.\n\n### Dataset de Entrenamiento\n- **Uso**: Este conjunto de datos se utiliza para entrenar el modelo. A través de este proceso, el modelo aprende a identificar patrones y a hacer predicciones.\n- **Importancia**: Es el mayor conjunto de datos y es crucial para que el modelo aprenda de manera efectiva.\n\n### Dataset de Validación\n- **Uso**: Se utiliza para validar el rendimiento del modelo en comparación con el conjunto de entrenamiento. Este conjunto ayuda a ajustar los hiperparámetros y evaluar el sobreajuste.\n- **Importancia**: Permite hacer ajustes en el modelo durante el entrenamiento, antes de la evaluación final en el conjunto de prueba.\n\n### Dataset de Prueba\n- **Uso**: Este conjunto se utiliza para probar las predicciones del modelo después de que ha sido entrenado y validado.\n- **Importancia**: Ofrece una evaluación final y objetiva del rendimiento del modelo en datos no vistos previamente, lo que es crucial para determinar la generalización del modelo.\n\nLa correcta separación y uso de estos conjuntos de datos es fundamental para desarrollar un modelo de Machine Learning que sea preciso, generalizable y robusto.\n","metadata":{}},{"cell_type":"code","source":"# Separación de set de entrenamiento y prueba (Después se hace la separación en entrenamiento, validación y pruena con el 90% del dataset)\nTRAIN_RATIO = 0.9 \nTEST_RATIO = 0.1\nDATASET_SIZE = len(X)\n\n# Se crean los conjuntos de entrenamiento que equivalen al 80% del conjunto de datos\nX2 = X[:int(DATASET_SIZE*TRAIN_RATIO)]\ny2 = y[:int(DATASET_SIZE*TRAIN_RATIO)]\nprint(X2.shape)\nprint(y2.shape)\n\n# Se crean los conjuntos de prueba que equivalen al 10% del conjunto de datos\nX_test = X[int(DATASET_SIZE*(TRAIN_RATIO)):]\ny_test = y[int(DATASET_SIZE*(TRAIN_RATIO)):]\nprint(X_test.shape)\nprint(y_test.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:56.170822Z","iopub.execute_input":"2023-12-13T07:13:56.171175Z","iopub.status.idle":"2023-12-13T07:13:56.185229Z","shell.execute_reply.started":"2023-12-13T07:13:56.171144Z","shell.execute_reply":"2023-12-13T07:13:56.184077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## K-FOLDS","metadata":{}},{"cell_type":"markdown","source":"### Fase de Compilación del Modelo\n\nEn la fase de compilación del modelo, establecemos los parámetros clave que guiarán el proceso de entrenamiento. Estos parámetros incluyen el optimizador, la función de pérdida y las métricas de evaluación.\n\n- **Optimizador Adam**: Seleccionamos el optimizador Adam con una tasa de aprendizaje específica, considerada baja. Esta elección es importante porque una tasa de aprendizaje más baja asegura que el modelo avance gradualmente durante el entrenamiento, evitando saltos demasiado grandes que podrían llevar a perder el mínimo óptimo en la función de pérdida.\n\n- **Función de Pérdida de Entropía Cruzada Binaria**: Esta función es adecuada para problemas de clasificación binaria. Funciona comparando las etiquetas predichas del modelo con las etiquetas verdaderas, proporcionando una medida cuantitativa de la pérdida (o error) que el modelo debe minimizar durante el entrenamiento.\n\n- **Métrica de Precisión**: Como métrica de evaluación, utilizamos la precisión. Esto nos permite medir el porcentaje de instancias que el modelo ha clasificado correctamente, proporcionando una visión clara de la efectividad del modelo durante el entrenamiento y la validación.\n\nEstos componentes son esenciales para asegurar que el modelo se entrene de manera eficiente y efectiva, optimizando su rendimiento y garantizando que los resultados sean confiables y precisos.\n","metadata":{}},{"cell_type":"code","source":"# Número de splits para KFold\nn_splits = 5\n\n# Inicializar KFold\nkf = KFold(n_splits=n_splits, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:56.186518Z","iopub.execute_input":"2023-12-13T07:13:56.187547Z","iopub.status.idle":"2023-12-13T07:13:56.193755Z","shell.execute_reply.started":"2023-12-13T07:13:56.187519Z","shell.execute_reply":"2023-12-13T07:13:56.192894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelos_entrenados = []\nhistorias = []\nresultados = []\n\nfor train_index, val_index in kf.split(X2):\n    # División de los datos para el fold actual\n    X_train_fold, X_val_fold = tf.gather(X2, train_index), tf.gather(X2, val_index)\n    y_train_fold, y_val_fold = tf.gather(y2, train_index), tf.gather(y2, val_index)\n\n    # Normalizar los datos\n    normalizador.adapt(X_train_fold)\n    X_train_fold = normalizador(X_train_fold)\n    X_val_fold = normalizador(X_val_fold)\n\n    # Crear una nueva instancia del modelo y compilarla\n    model = crear_modelo()  \n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n                  loss=tf.keras.losses.BinaryCrossentropy(),\n                  metrics=['accuracy'])\n    # Definición del callback de EarlyStopping\n    early_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=int(50),# Valor de ajuste\n    verbose=1,\n    restore_best_weights=True\n)\n\n    # Entrenar el modelo con EarlyStopping\n    history = model.fit(\n        X_train_fold, \n        y_train_fold, \n        validation_data=(X_val_fold, y_val_fold), \n        epochs=1000,\n        verbose=1,\n        callbacks=[early_stopping]\n    )\n    \n    # Guardar el modelo entrenado en la lista\n    modelos_entrenados.append(model)\n    \n    # Guardar la historia de este fold\n    historias.append(history)\n\n    # Evaluar el modelo y guardar los resultados\n    resultado = model.evaluate(X_val_fold, y_val_fold)\n    resultados.append(resultado)\n\n    # Limpieza de memoria si es necesario\n    tf.keras.backend.clear_session()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:13:56.195031Z","iopub.execute_input":"2023-12-13T07:13:56.195334Z","iopub.status.idle":"2023-12-13T07:22:01.240395Z","shell.execute_reply.started":"2023-12-13T07:13:56.195309Z","shell.execute_reply":"2023-12-13T07:22:01.239354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convertir los resultados a un array de NumPy para facilitar el cálculo\nresultados_np = np.array(resultados)\n\n# Calcular promedio y desviación estándar para cada métrica\npromedios = np.mean(resultados_np, axis=0)\ndesviaciones_estandar = np.std(resultados_np, axis=0)\n\n# Imprimir los resultados\nprint(\"Promedio de las métricas por fold:\")\nfor i, metrica in enumerate(model.metrics_names):\n    print(f\"{metrica}: {promedios[i]:.4f} ± {desviaciones_estandar[i]:.4f}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:22:01.245886Z","iopub.execute_input":"2023-12-13T07:22:01.246238Z","iopub.status.idle":"2023-12-13T07:22:01.254603Z","shell.execute_reply.started":"2023-12-13T07:22:01.246209Z","shell.execute_reply":"2023-12-13T07:22:01.253539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Graficar la pérdida para cada fold\nfor i, history in enumerate(historias):\n    plt.figure(figsize=(8, 5))\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title(f'Model Loss - Fold {i+1}')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(loc='upper right')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:22:01.255780Z","iopub.execute_input":"2023-12-13T07:22:01.256112Z","iopub.status.idle":"2023-12-13T07:22:02.576623Z","shell.execute_reply.started":"2023-12-13T07:22:01.256085Z","shell.execute_reply":"2023-12-13T07:22:02.575616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encontrar el índice del mejor modelo (p. ej., el que tiene la menor pérdida de validación)\nindice_mejor_modelo = np.argmin([resultado[0] for resultado in resultados])  # Asumiendo que la posición 0 es la pérdida\n\n# Seleccionar el mejor modelo\nmejor_modelo = modelos_entrenados[indice_mejor_modelo]\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:22:02.578245Z","iopub.execute_input":"2023-12-13T07:22:02.579000Z","iopub.status.idle":"2023-12-13T07:22:02.584891Z","shell.execute_reply.started":"2023-12-13T07:22:02.578959Z","shell.execute_reply":"2023-12-13T07:22:02.583727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(indice_mejor_modelo)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:24:08.728997Z","iopub.execute_input":"2023-12-13T07:24:08.729384Z","iopub.status.idle":"2023-12-13T07:24:08.734873Z","shell.execute_reply.started":"2023-12-13T07:24:08.729354Z","shell.execute_reply":"2023-12-13T07:24:08.733843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Iterar sobre cada historia de entrenamiento y graficar la precisión\nfor i, history in enumerate(historias):\n    plt.figure(figsize=(8, 5))\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title(f'Model Accuracy - Fold {i+1}')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(loc='lower right')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:22:02.586356Z","iopub.execute_input":"2023-12-13T07:22:02.586773Z","iopub.status.idle":"2023-12-13T07:22:03.821300Z","shell.execute_reply.started":"2023-12-13T07:22:02.586731Z","shell.execute_reply":"2023-12-13T07:22:03.820298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluar el modelo sobre el  dataset de entrenamiento\n# Esto proporcionará el valor de pérdida y la métrica de precisión para el conjunto de prueba\n# Indicando que tan bien se comporta el modelo con nuevos datos\ntest_loss, test_accuracy = mejor_modelo.evaluate(X_test, y_test)\n\n# Imprimir los resultados de prueba\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Test Accuracy: {test_accuracy}\")\n\n# Evaluar el modelo sobre el dataset de validación\n# Similar a la evaluación de prueba, esto proporcionará el valor de pérdida y la métrica de precisión\n# para los datos de validación, los cuales pueden ser monitoreados para el overfitting durante el entrenamiento.\nval_loss, val_accuracy = mejor_modelo.evaluate(X_val_fold, y_val_fold)\n\n# Imprimir los resultados de validación\nprint(f\"Validation Loss: {val_loss}\")\nprint(f\"Validation Accuracy: {val_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:25:02.235382Z","iopub.execute_input":"2023-12-13T07:25:02.236266Z","iopub.status.idle":"2023-12-13T07:25:02.617586Z","shell.execute_reply.started":"2023-12-13T07:25:02.236229Z","shell.execute_reply":"2023-12-13T07:25:02.616338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extraer los valores verdaderos (etiquetas reales) de los datos de prueba.\n# Convertir el tensor a arreglos numpy y posteriormente a listas para una manipulación mas eficiente. \ny_true = list(y_test[:, 0].numpy())\n\n# Usar el modelo para realizar predicciones sobre los datos de prueba\n# Después, extraer la primera columna de predicciones (asumiendo que es la columna de interes).\n# Convertir el tensor de predicción a una lista\ny_pred = list(mejor_modelo.predict(X_test)[:, 0])\n\n# Imprimir las predicciones hechas por el modelo\nprint(y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:25:08.733805Z","iopub.execute_input":"2023-12-13T07:25:08.734760Z","iopub.status.idle":"2023-12-13T07:25:08.967921Z","shell.execute_reply.started":"2023-12-13T07:25:08.734719Z","shell.execute_reply":"2023-12-13T07:25:08.966927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Convierte y_pred y y_true a arrays de NumPy si aún no lo son\ny_pred = np.array(y_pred)\ny_true = np.array(y_true)\n\n# Asegurar que y_pred y y_true tengan la misma longitud\nassert len(y_pred) == len(y_true), \"Length mismatch between y_pred and y_true.\"\n\n# Calcula el tamaño de la muestra como el 10% del total\nsample_size = int(len(y_pred) * 0.1)\n\n# Selecciona índices aleatorios para representar el 10% de los datos\nindices = np.random.choice(len(y_pred), sample_size, replace=False)\n\n# Filtra los datos según los índices seleccionados\ny_pred_sample = y_pred[indices]\ny_true_sample = y_true[indices]\nind_sample = np.arange(sample_size)  # Ajusta el array ind para la muestra\n\n# Configura el gráfico\nplt.figure(figsize=(20, 10))  # Ajusta el tamaño de la figura según sea necesario\nwidth = 0.4  # Ajusta el ancho de las barras\n\n# Dibuja las barras para los datos filtrados\nplt.bar(ind_sample, y_pred_sample, width, label='Predicted Transportation')\nplt.bar(ind_sample + width, y_true_sample, width, label='Actual Transportation')\n\n# Etiquetas y leyenda\nplt.xlabel('Sample Index')\nplt.ylabel('Spacer Titanic Transportation')\nplt.title('Comparison of Actual vs Predicted Transportation (10% Sample)')\nplt.legend()\n\n# Muestra el gráfico\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:25:12.526430Z","iopub.execute_input":"2023-12-13T07:25:12.526829Z","iopub.status.idle":"2023-12-13T07:25:13.209762Z","shell.execute_reply.started":"2023-12-13T07:25:12.526798Z","shell.execute_reply":"2023-12-13T07:25:13.208681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calcular los puntos de la curva ROC y la puntuación AUC\nfpr, tpr, _ = roc_curve(y_true, y_pred)\nroc_auc = auc(fpr, tpr)\n\n# Graficar la curva ROC\nplt.figure(figsize=(10, 5))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:25:18.496756Z","iopub.execute_input":"2023-12-13T07:25:18.497209Z","iopub.status.idle":"2023-12-13T07:25:18.734082Z","shell.execute_reply.started":"2023-12-13T07:25:18.497177Z","shell.execute_reply":"2023-12-13T07:25:18.732934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calcular la matriz de confusión\ncm = confusion_matrix(y_true, np.round(y_pred))\n\n# Graficar la matriz de confusión\nplt.figure(figsize=(5, 5))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap=plt.cm.Blues)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:25:25.632286Z","iopub.execute_input":"2023-12-13T07:25:25.632741Z","iopub.status.idle":"2023-12-13T07:25:25.941291Z","shell.execute_reply.started":"2023-12-13T07:25:25.632709Z","shell.execute_reply":"2023-12-13T07:25:25.940278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calcular la precisión y la recuperación para diferentes umbrales de decisión\nprecision, recall, _ = precision_recall_curve(y_true, y_pred)\n\n# Graficar la curva de precisión-recuperación\nplt.figure(figsize=(10, 5))\ndisp = PrecisionRecallDisplay(precision=precision, recall=recall)\ndisp.plot()\nplt.title('Precision-Recall curve')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:27:00.846147Z","iopub.execute_input":"2023-12-13T07:27:00.846886Z","iopub.status.idle":"2023-12-13T07:27:01.125724Z","shell.execute_reply.started":"2023-12-13T07:27:00.846831Z","shell.execute_reply":"2023-12-13T07:27:01.124743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tensor_data_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:27:04.875134Z","iopub.execute_input":"2023-12-13T07:27:04.875842Z","iopub.status.idle":"2023-12-13T07:27:04.881973Z","shell.execute_reply.started":"2023-12-13T07:27:04.875809Z","shell.execute_reply":"2023-12-13T07:27:04.881005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(df_test))\nprint(len(tensor_data_test))","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:27:05.784802Z","iopub.execute_input":"2023-12-13T07:27:05.785192Z","iopub.status.idle":"2023-12-13T07:27:05.790921Z","shell.execute_reply.started":"2023-12-13T07:27:05.785163Z","shell.execute_reply":"2023-12-13T07:27:05.789917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hacer predicciones con el mejor modelo\npredictions = mejor_modelo.predict(tensor_data_test).flatten()\n\n# Imprime el número de predicciones para asegurarte de que coincide con el número esperado de instancias de prueba.\nprint(len(predictions))","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:27:07.188041Z","iopub.execute_input":"2023-12-13T07:27:07.188412Z","iopub.status.idle":"2023-12-13T07:27:07.533745Z","shell.execute_reply.started":"2023-12-13T07:27:07.188384Z","shell.execute_reply":"2023-12-13T07:27:07.532669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hacer predicciones con el mejor modelo\npredictions = mejor_modelo.predict(tensor_data_test).flatten()\n\n# Convertir las probabilidades predichas en valores booleanos (True o False)\n# Aplicando un umbral: las predicciones >= 0.5 se convierten en True, de lo contrario en False\nbinary_predictions = [True if pred >= 0.5 else False for pred in predictions]\n\n# Crear una nueva columna combinando PassengerGroupID y PassengerNumber con un '_'\n# asegurándose de que PassengerGroupID tenga 4 dígitos y PassengerNumber tenga 2 dígitos\ndf_test['CombinedID'] = df_test['PassengerGroupID'].astype(str).str.zfill(4) + '_' + df_test['PassengerNumber'].astype(str).str.zfill(2)\n\n# Crear el DataFrame output con el CombinedID y las predicciones\noutput = pd.DataFrame({'PassengerId': df_test['CombinedID'], 'Transported': binary_predictions})\n\n\n# Save the DataFrame to a CSV file to be used as a submission file\noutput.to_csv(\"submission.csv\", index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2023-12-13T07:27:11.400611Z","iopub.execute_input":"2023-12-13T07:27:11.401628Z","iopub.status.idle":"2023-12-13T07:27:11.760274Z","shell.execute_reply.started":"2023-12-13T07:27:11.401590Z","shell.execute_reply":"2023-12-13T07:27:11.759321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}